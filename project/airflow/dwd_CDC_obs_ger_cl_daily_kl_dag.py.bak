import glob
import logging
import os
import re
import shutil
import urllib.request
import zipfile
from datetime import datetime
from email.policy import default
from fileinput import filename
from pathlib import Path
from xml.dom.pulldom import START_DOCUMENT

import pyarrow as pa
import pyarrow.csv as pv
import pyarrow.parquet as pq
from airflow import DAG
from airflow.models.baseoperator import chain
from airflow.utils.dates import days_ago
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.bigquery import \
    BigQueryCreateExternalTableOperator, BigQueryInsertJobOperator
from google.cloud import storage

from helper_fun import (download_all_datafiles, format_all_csv_to_parquet, format_to_parquet,
                        get_all_filenames, log_values, station_data_to_csv,
                        upload_parquet_to_gcs)


### Define default arguments and variables
AIRFLOW_HOME = os.environ.get('AIRFLOW_HOME', '/opt/airflow/')

PROJECT_ID = os.environ.get('GCP_PROJECT_ID')
BUCKET = os.environ.get('GCP_GCS_BUCKET')
BIGQUERY_DATASET = os.environ.get('BIGQUERY_DATASET', 'weather_data_all')

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 0,
    'max_active_runs': 1,
    'catchup': True, 
}



# URL_PREFIX =  'https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/'
# TIME_WINDOW = ['historical/', 'recent/']
# FILENAME_STATION_DATA = 'KL_Tageswerte_Beschreibung_Stationen.txt'
# FILENAME_STATION_DATA_2 = 'KL_Tageswerte_mn4_Beschreibung_Stationen.txt'
# FILENAME_BASE_HIST = 'tageswerte_KL_00001_19370101_19860630_hist.zip'
# FILENAME_CSV = FILENAME_BASE + '.csv'
# FILENAME_PARQUET = FILENAME_BASE + '.parquet'
# URL_TEMPLATE = URL_PREFIX + FILENAME_CSV

TEMP_DIR = './temp/'
URL_PREFIX =  'https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/'
PRODUCT_CATEGORY = ['kl/']
TIME_RANGE = ['historical/', 'recent/']
SCHEDULE_INTERVAL = ['0 6 1 * *', '0 6 * * *']
DATA_FOLDER = './data/'
INDEX = 'index.html'
AVAIL_FILES = 'avail_files.txt'
FILENAME_STATION_DATA = 'KL_Tageswerte_Beschreibung_Stationen.txt'
FILENAME_STATION_DATA_2 = 'KL_Tageswerte_mn4_Beschreibung_Stationen.txt'
FILENAME_BASE_HIST = 'tageswerte_KL_00001_19370101_19860630_hist.zip'
FILENAME_BASE = 'produkt_klima_tag'

INPUT_FILETYPE = 'parquet'

TABLE_SCHEMA = [
            ('STATIONS_ID',  pa.int64()),
            ('MESS_DATUM',  pa.int64()), 
            ('QN_3',  pa.int32()),  # quality level
            ('FX',  pa.float64()),   # max of wind gust
            ('FM',  pa.float64()),    # mean wind
            ('QN_4',  pa.int32()),  # quality level of next column
            ('RSK',  pa.float64()), # daily precipitation
            ('RSKF',  pa.int32()),  # precipitation form
            ('SDK',  pa.float64()),  # sunshine duration
            ('SHK_TAG',  pa.float64()),   # snow height (cm)
            ('NM',  pa.float64()),   # mean cloud cover
            ('VPM',  pa.float64()),  # mean vapor pressure
            ('PM',  pa.float64()),   # mean pressure
            ('TMK',  pa.float64()),  # mean temp
            ('UPM',  pa.float64()),  # mean humidity
            ('TXK',  pa.float64()),  # max temp
            ('TNK',  pa.float64()),  # min temp
            ('TGK',  pa.float64()),  # min temp (5cm)
            ('eor',  pa.string()),  # end of data record
        ]



### Creating a dag with schedule_interval and time_range as parameter
def create_dag(dag_id=f'dwd_dag',
               schedule_interval=SCHEDULE_INTERVAL[1], 
               start_date=days_ago(1),
               time_range=TIME_RANGE[1],
               default_args=default_args,
               tags=[]):

    dag = DAG(dag_id,
              schedule_interval=schedule_interval,
              default_args=default_args,
              start_date=start_date,
              tags=tags
              )
    with dag:

        datadir = 'data_'+time_range

        # Create data directory
        create_data_dir_task = BashOperator(
            task_id=f'create_data_dir_{time_range.strip("/")}_task',
            bash_command=f'rm -rf {AIRFLOW_HOME}/{datadir} && mkdir {AIRFLOW_HOME}/{datadir}'
        )

        # Download the station data
        download_station_data_task = PythonOperator(
            task_id=f'download_station_data_{time_range.strip("/")}_task',
            python_callable=station_data_to_csv,
            op_kwargs={
                'baseurl': f'{URL_PREFIX}{PRODUCT_CATEGORY[0]}{time_range}',
                'datadir': datadir,
                'filename': FILENAME_STATION_DATA,
            }
        )

        # Format station data to parquet
        format_station_data_to_parquet_task = PythonOperator(
            task_id=f'format_station_data_to_parquet_{time_range.strip("/")}_task',
            python_callable=format_to_parquet,
            op_kwargs={
                'src_file': FILENAME_STATION_DATA,
                'datadir': datadir,
                'delimiter': ';',
            }
        )

        # Get the filenames of available files
        get_all_filenames_task = PythonOperator(
            task_id=f'get_all_filenames_{time_range.strip("/")}_task',
            python_callable=get_all_filenames,
            op_kwargs={
                'url': f'{URL_PREFIX}{PRODUCT_CATEGORY[0]}{time_range}', 
                'datadir': datadir,
                'filename_file': AVAIL_FILES,
            }
        )

        # Download the data 
        download_datafiles_task = PythonOperator(
            task_id=f'download_datafiles_{time_range.strip("/")}_task',
            python_callable=download_all_datafiles,
            op_kwargs={
                'base_url': f'{URL_PREFIX}{PRODUCT_CATEGORY[0]}{time_range}',
                'datadir': datadir,
                'filename_file': AVAIL_FILES,
            }
        )

        # Format to parquet
        format_all_csv_to_parquet_task = PythonOperator(
            task_id=f'format_all_csv_to_parquet_{time_range.strip("/")}_task',
            python_callable=format_all_csv_to_parquet,    
            op_kwargs={
                'datadir': datadir,
                'table_schema': TABLE_SCHEMA,
                'delimiter': ';',
            }    
        )

        # Delete all csv files
        delete_csv_task = BashOperator(
            task_id=f'delete_csv_{time_range.strip("/")}_task',
            bash_command=f'rm -f {AIRFLOW_HOME}/{datadir}/*.csv'
        )

        # Move files to google cloud storage
        local_to_gcs_task = PythonOperator(
            task_id=f'local_to_gcs_{time_range.strip("/")}_task',
            python_callable=upload_parquet_to_gcs,
            op_kwargs={
                'bucket': BUCKET,
                'datadir': datadir,
            },
        )

        # Clean up
        delete_datadir_task = BashOperator(
            task_id=f'delete_datadir_{time_range.strip("/")}_task',
            bash_command=f'rm -rf {AIRFLOW_HOME}/{datadir}'
        )

        # Create external table for station data in Big Query
        bigquery_external_table_station_data_task = BigQueryCreateExternalTableOperator(
            task_id=f'bigquery_external_table_station_data_{time_range.strip("/")}_task',
            table_resource={
                'tableReference': {
                    'projectId': PROJECT_ID,
                    'datasetId': BIGQUERY_DATASET,
                    'tableId': f'{PRODUCT_CATEGORY[0].strip("/")}_{time_range.strip("/")}_station_data_external_table',
                },
                'externalDataConfiguration': {
                    'autodetect': 'True',
                    'sourceFormat': f'{INPUT_FILETYPE.upper()}',
                    'sourceUris': [f'gs://{BUCKET}/raw/{datadir}{FILENAME_STATION_DATA.strip(".txt")}*'],
                },
            },
        )

        # Create partitioned table for station data in Big Query
        CREATE_BQ_TBL_QUERY_STATION_DATA = (f'CREATE OR REPLACE TABLE \
            {BIGQUERY_DATASET}.{PRODUCT_CATEGORY[0].strip("/")}_{time_range.strip("/").lower()}_station_data_internal_table \
                AS (\
                    SELECT * FROM \
                        {BIGQUERY_DATASET}.{PRODUCT_CATEGORY[0].strip("/")}_{time_range.strip("/")}_station_data_external_table)'
        )

        # Create a partitioned table for weather data from external table
        bigquery_create_internal_table_station_data_task = BigQueryInsertJobOperator(
            task_id=f'bigquery_create_internal_table_station_data_{time_range.strip("/")}_task',
            configuration={
                'query': {
                    'query': CREATE_BQ_TBL_QUERY_STATION_DATA,
                    'useLegacySql': False,
                }
            }
        )

        # Create external table for weather data in Big Query
        bigquery_external_table_task = BigQueryCreateExternalTableOperator(
            task_id=f'bigquery_external_table_{time_range.strip("/")}_task',
            table_resource={
                'tableReference': {
                    'projectId': PROJECT_ID,
                    'datasetId': BIGQUERY_DATASET,
                    'tableId': f'{PRODUCT_CATEGORY[0].strip("/")}_{time_range.strip("/")}_external_table',
                },
                'externalDataConfiguration': {
                    'autodetect': 'True',
                    'sourceFormat': f'{INPUT_FILETYPE.upper()}',
                    'sourceUris': [f'gs://{BUCKET}/raw/{datadir}{FILENAME_BASE}*'],
                },
            },
        )

        # Create partitioned table for weather data in Big Query
        CREATE_BQ_TBL_QUERY = (f'CREATE OR REPLACE TABLE \
            {BIGQUERY_DATASET}.{PRODUCT_CATEGORY[0].strip("/")}_{time_range.strip("/").lower()}_partitioned_table \
                CLUSTER BY STATIONS_ID AS (\
                    SELECT * FROM \
                        {BIGQUERY_DATASET}.{PRODUCT_CATEGORY[0].strip("/")}_{time_range.strip("/")}_external_table)'
        )

        # Logging task
        logging_task = PythonOperator(
            task_id='logging_task',
            python_callable=log_values,
            op_kwargs={
                'params': [f'{PRODUCT_CATEGORY[0].strip("/")}_{time_range.strip("/")}_external_table',
                f'gs://{BUCKET}/raw/{datadir}*', CREATE_BQ_TBL_QUERY]
            }
        )

        # Create a partitioned table for weather data from external table
        bigquery_create_partitioned_table_task = BigQueryInsertJobOperator(
            task_id=f'bigquery_create_partitioned_table_{time_range.strip("/")}_task',
            configuration={
                'query': {
                    'query': CREATE_BQ_TBL_QUERY,
                    'useLegacySql': False,
                }
            }
        )

        chain(
            logging_task, 
        )

        chain(
            create_data_dir_task, 
            download_station_data_task,
            format_station_data_to_parquet_task,
            delete_csv_task, 
            local_to_gcs_task, 
            delete_datadir_task, 
            bigquery_external_table_station_data_task, 
            bigquery_create_internal_table_station_data_task,
        )

        chain(
            create_data_dir_task, 
            get_all_filenames_task, 
            download_datafiles_task, 
            format_all_csv_to_parquet_task, 
            delete_csv_task, 
            local_to_gcs_task, 
            delete_datadir_task, 
            bigquery_external_table_task, 
            bigquery_create_partitioned_table_task,
        )

        # chain(
        #     create_data_dir_task, 
        #     download_station_data_task,
        #     format_station_data_to_parquet_task,
        #     get_all_filenames_task, 
        #     download_datafiles_task, 
        #     format_all_csv_to_parquet_task, 
        #     delete_csv_task, 
        #     local_to_gcs_task, 
        #     delete_datadir_task, 
        #     logging_task, 
        #     bigquery_external_table_station_data_task, 
        #     bigquery_external_table_task, 
        #     bigquery_create_partitioned_table_task,
        # )

    return dag

### Create two dags, one for the historical and one for the recent data
for time_range, schedule_interval in zip(TIME_RANGE, SCHEDULE_INTERVAL):
    dag_id=f'dwd_CDC_obs_ger_cl_daily_kl_{time_range.strip("/")}_dag'
    
    globals()[dag_id] = create_dag(dag_id,
                                   schedule_interval=schedule_interval,
                                   start_date=datetime.now(),
                                   time_range=time_range,
                                   default_args=default_args,
                                   tags=['weather', time_range],)



### DAG for downloading the recent weather data
# NOTE: DAG declaration - using a Context Manager (an implicit way)

# with DAG(
#     dag_id=f'dwd_CDC_obs_ger_cl_daily_kl_{time_range.strip("/")}_dag',
#     schedule_interval=SCHEDULE_INTERVAL[1],
#     start_date=datetime.now(),
#     default_args=default_args,
#     max_active_runs=1,
#     catchup=True, 
#     tags=['weather_project']
# ) as dag:

#     for time_range, schedule_interval in zip(TIME_RANGE, SCHEDULE_INTERVAL):

#         # Download the station data
#         download_station_data_task = PythonOperator(
#             task_id=f'download_station_data_{time_range.strip("/")}_task',
#             python_callable=station_data_to_csv,
#             op_kwargs={
#                 'url': f'{URL_PREFIX}{PRODUCT_CATEGORY[0]}{time_range}',
#                 'datadir': time_range,
#                 'filename': FILENAME_STATION_DATA,
#             }
#         )

#         # Get the filenames of available files
#         get_all_filenames_task = PythonOperator(
#             task_id=f'get_all_filenames_{time_range.strip("/")}_task',
#             python_callable=get_all_filenames,
#             op_kwargs={
#                 'base_url': f'{URL_PREFIX}{PRODUCT_CATEGORY[0]}{time_range}', 
#                 'datadir': time_range,
#                 'filename_file': AVAIL_FILES,
#             }
#         )

#         # Download the data 
#         download_datafiles_task = PythonOperator(
#             task_id=f'download_datafiles_{time_range.strip("/")}_task',
#             python_callable=download_all_datafiles,
#             op_kwargs={
#                 'base_url': f'{URL_PREFIX}{PRODUCT_CATEGORY[0]}{time_range}',
#                 'datadir': time_range,
#                 'filename_file': AVAIL_FILES,
#             }
#         )

#         # Format to parquet
#         format_csv_to_parquet_task = PythonOperator(
#             task_id=f'format_csv_to_parquet_{time_range.strip("/")}_task',
#             python_callable=format_csv_to_parquet,    
#             op_kwargs={
#                 'datadir': time_range,
#             }    
#         )

#         delete_csv_task = BashOperator(
#             task_id=f'delete_csv_{time_range.strip("/")}_task',
#             bash_command=f'rm -f {AIRFLOW_HOME}/*.csv'
#         )

#         local_to_gcs_task = PythonOperator(
#             task_id=f'local_to_gcs_{time_range.strip("/")}_task',
#             python_callable=upload_parquet_to_gcs,
#             op_kwargs={
#                 'bucket': BUCKET,
#             },
#         )

#         delete_datadir_task = BashOperator(
#             task_id=f'delete_datadir_{time_range.strip("/")}_task',
#             bash_command=f'rm -f {AIRFLOW_HOME}/{time_range}'
#         )


#         download_station_data_task >> get_all_filenames_task >> download_datafiles_task >> \
#             format_csv_to_parquet_task >> delete_csv_task >> local_to_gcs_task >> \
#             delete_datadir_task






        # download_overview_data_task = PythonOperator(
        #     task_id='download_overview_data_task'
        #     python_callable=
        # )

        # download_station_data_task = BashOperator(
        #     task_id='download_station_data_task'
        #     bash_command=f'curl -sSLf {URL_TEMPLATE} > {AIRFLOW_HOME}/{FILENAME_CSV}'
        # )
        # download_dataset_task = BashOperator(
        #     task_id='download_dataset_task',
        #     bash_command=f'curl -sSLf {URL_TEMPLATE} > {AIRFLOW_HOME}/{FILENAME_CSV}'
        # )

        # delete_csv_task = BashOperator(
        #     task_id='delete_csv_task',
        #     bash_command=f'rm -f {AIRFLOW_HOME}/{FILENAME_CSV}'
        # )

        # format_to_parquet_task = PythonOperator(
        #     task_id='format_to_parquet_task',
        #     python_callable=format_to_parquet,
        #     op_kwargs={
        #         'src_file': f'{AIRFLOW_HOME}/{FILENAME_CSV}',
        #     },
        # )

        # local_to_gcs_task = PythonOperator(
        #     task_id='local_to_gcs_task',
        #     python_callable=upload_to_gcs,
        #     op_kwargs={
        #         'bucket': BUCKET,
        #     },
        # )

        # delete_parquet_task = BashOperator(
        #     task_id='delete_parquet_task',
        #     bash_command=f'rm -f {AIRFLOW_HOME}/{FILENAME_PARQUET}'
        # )

        # download_dataset_task >> format_to_parquet_task >> delete_csv_task >> local_to_gcs_task >> delete_parquet_task

